{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhmeharbedi/102165002-SESS_LE1/blob/main/102165002_PRABHMEHAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Introduction and Paper Summary\n",
        "\n",
        "# Speech Commands Classification - Lab Evaluation\n",
        "\n",
        "This notebook follows the evaluation tasks based on the paper\n",
        "[Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition]\n",
        "(https://arxiv.org/abs/1804.03209).\n",
        "\n",
        "### Tasks:\n",
        "1. Summarize the paper in about 50 words.\n",
        "2. Download, analyze, and statistically describe the dataset.\n",
        "3. Train a classifier to distinguish commands.\n",
        "4. Report performance results using standard benchmarks.\n",
        "5. Record 30 samples of each command in your voice and create a new dataset.\n",
        "6. Fine-tune the classifier on your voice.\n",
        "7. Report the results.\n",
        "\n",
        "## 1. Paper Summary\n",
        "\n",
        "The paper introduces a labeled dataset consisting of one-second audio clips of simple spoken words.\n",
        "It is designed to facilitate research in limited-vocabulary speech recognition tasks.\n",
        "The dataset contains a variety of speakers and environments, providing a robust benchmark for training machine learning models."
      ],
      "metadata": {
        "id": "67-K4SqzVjyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZsARhtADn183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94803ad5-7d87-4e18-bc81-61eb4e854e28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to calculate MD5 checksum of a file\n",
        "import hashlib\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "# Example usage: Calculate and verify the checksum of the dataset\n",
        "dataset_file = '/content/drive/MyDrive/Keyword Spotting/Recordings/yes_1.wav'\n",
        "dataset_checksum = calculate_md5(dataset_file)\n",
        "print(f'MD5 checksum for dataset: {dataset_checksum}')\n",
        "\n",
        "# Store this checksum and use it for later verification\n",
        "# Verifying the file later\n",
        "new_checksum = calculate_md5(dataset_file)\n",
        "if new_checksum == dataset_checksum:\n",
        "    print(\"File is verified and hasn't been tampered with.\")\n",
        "else:\n",
        "    print(\"File verification failed. The file has been modified.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3HcMG6DzN9i",
        "outputId": "8a144c46-f5ed-4c72-be02-7181a725bd3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD5 checksum for dataset: b18b2c3f131b2577e71b36076a2a6927\n",
            "File is verified and hasn't been tampered with.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analyze Dataset"
      ],
      "metadata": {
        "id": "qsVDXSTdWLZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download and Analyze the Dataset\n",
        "import os\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "FdyOuBQYVRq2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data directory if it doesn't exist\n",
        "data_dir = './data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ],
      "metadata": {
        "id": "6TD1BbWwV6Pp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Speech Commands dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIZA4kbOV_rJ",
        "outputId": "a1185c9a-f52f-4344-cf83-6f3c748372a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [01:47<00:00, 22.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 10 commands to work with\n",
        "selected_commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'go', 'stop', 'on', 'off']\n",
        "\n",
        "# Limit the number of samples per command (e.g., 100 samples per command)\n",
        "samples_per_command = 100"
      ],
      "metadata": {
        "id": "7mmyB28qWCYt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subset of the dataset by filtering for the selected commands\n",
        "subset_indices = []\n",
        "command_counter = Counter()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "    label = sample[2]\n",
        "    if label in selected_commands and command_counter[label] < samples_per_command:\n",
        "        subset_indices.append(idx)\n",
        "        command_counter.update([label])\n",
        "\n",
        "    # Stop when we have enough samples for each command\n",
        "    if all(command_counter[cmd] >= samples_per_command for cmd in selected_commands):\n",
        "        break\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Check the sample count for each command in the subset\n",
        "print(f\"Sample counts in subset: {command_counter}\")\n",
        "print(f\"Total subset size: {len(subset_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLsG5in0WFl7",
        "outputId": "033b9241-007e-4356-cf01-8f6d94186ba2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample counts in subset: Counter({'down': 100, 'go': 100, 'left': 100, 'no': 100, 'off': 100, 'on': 100, 'right': 100, 'stop': 100, 'up': 100, 'yes': 100})\n",
            "Total subset size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "N0JTonRtW6nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Preprocessing (Padding and Truncating)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "9hf5wYt0VSMN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a fixed length for all audio samples (1 second = 16000 samples at 16kHz)\n",
        "fixed_length = 16000\n",
        "\n",
        "# Custom collate function to pad and truncate audio data\n",
        "def collate_fn(batch):\n",
        "    waveforms = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        waveform = item[0]\n",
        "        label = item[2]\n",
        "\n",
        "        if waveform.shape[1] > fixed_length:\n",
        "            waveform = waveform[:, :fixed_length]\n",
        "        elif waveform.shape[1] < fixed_length:\n",
        "            pad_amount = fixed_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "        waveforms.append(waveform)\n",
        "        labels.append(label)\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    return waveforms, labels"
      ],
      "metadata": {
        "id": "g9T7BVULWfr_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the subset dataset\n",
        "loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "P5gI2cizWijE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. CNN Classifier"
      ],
      "metadata": {
        "id": "DHqsVkRBXEUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define and Train a CNN Classifier (with correct fully connected layer input size)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Define a simple CNN model for speech command classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 32, 128)  # Correct input size based on shape (32*25=800)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the number of classes (commands)\n",
        "num_classes = len(selected_commands)\n",
        "\n",
        "# Create a dictionary to map commands (labels) to numerical values\n",
        "label_to_index = {label: idx for idx, label in enumerate(selected_commands)}\n",
        "\n",
        "# Function to convert string labels to numerical indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN(num_classes=num_classes).to('cuda')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        # Convert waveforms to spectrograms\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')  # Remove extra dimension, add channel dimension\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print('Training completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3eSGNqXW0WH",
        "outputId": "ac62242f-0103-4f52-a969-fc78a65e39a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.601890716701746\n",
            "Epoch 2/50, Loss: 1.684557981789112\n",
            "Epoch 3/50, Loss: 1.3523731864988804\n",
            "Epoch 4/50, Loss: 1.1243946868926287\n",
            "Epoch 5/50, Loss: 0.996374910697341\n",
            "Epoch 6/50, Loss: 0.845909109339118\n",
            "Epoch 7/50, Loss: 0.6968596735969186\n",
            "Epoch 8/50, Loss: 0.5915448756422848\n",
            "Epoch 9/50, Loss: 0.5313666826114058\n",
            "Epoch 10/50, Loss: 0.6627821503207088\n",
            "Epoch 11/50, Loss: 0.539000429213047\n",
            "Epoch 12/50, Loss: 0.4645805268082768\n",
            "Epoch 13/50, Loss: 0.37597756111063063\n",
            "Epoch 14/50, Loss: 0.3524410950485617\n",
            "Epoch 15/50, Loss: 0.3254911662079394\n",
            "Epoch 16/50, Loss: 0.3025505947880447\n",
            "Epoch 17/50, Loss: 0.23821532796137035\n",
            "Epoch 18/50, Loss: 0.21791425510309637\n",
            "Epoch 19/50, Loss: 0.2132582104532048\n",
            "Epoch 20/50, Loss: 0.20353090862045065\n",
            "Epoch 21/50, Loss: 0.29704794858116657\n",
            "Epoch 22/50, Loss: 0.32159089646302164\n",
            "Epoch 23/50, Loss: 0.2791507130023092\n",
            "Epoch 24/50, Loss: 0.31834160501603037\n",
            "Epoch 25/50, Loss: 0.48384921508841217\n",
            "Epoch 26/50, Loss: 0.2793220744933933\n",
            "Epoch 27/50, Loss: 0.18945682770572603\n",
            "Epoch 28/50, Loss: 0.1482231748814229\n",
            "Epoch 29/50, Loss: 0.16532690648455173\n",
            "Epoch 30/50, Loss: 0.12271780264563859\n",
            "Epoch 31/50, Loss: 0.10771248006494716\n",
            "Epoch 32/50, Loss: 0.09968393240706064\n",
            "Epoch 33/50, Loss: 0.099469150591176\n",
            "Epoch 34/50, Loss: 0.08524026129452977\n",
            "Epoch 35/50, Loss: 0.07978747232118621\n",
            "Epoch 36/50, Loss: 0.07462470326572657\n",
            "Epoch 37/50, Loss: 0.07069821761979256\n",
            "Epoch 38/50, Loss: 0.06820142106153071\n",
            "Epoch 39/50, Loss: 0.06437604133679997\n",
            "Epoch 40/50, Loss: 0.065767371430411\n",
            "Epoch 41/50, Loss: 0.0590941212431062\n",
            "Epoch 42/50, Loss: 0.05442866370867705\n",
            "Epoch 43/50, Loss: 0.05106657472788356\n",
            "Epoch 44/50, Loss: 0.055146897269878536\n",
            "Epoch 45/50, Loss: 0.04946274420944974\n",
            "Epoch 46/50, Loss: 0.04686199167917948\n",
            "Epoch 47/50, Loss: 0.04176659083168488\n",
            "Epoch 48/50, Loss: 0.039794400276150554\n",
            "Epoch 49/50, Loss: 0.03917732057743706\n",
            "Epoch 50/50, Loss: 0.038035643294279\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation"
      ],
      "metadata": {
        "id": "ZzvfL4tuXIcl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfrQ_hlLVaH5",
        "outputId": "b67c3d81-0617-4c4f-a4a4-847ae88c7004"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Files in the recordings folder: ['no_22.wav', 'yes_1.wav', 'yes_19.wav', 'no_1.wav', 'yes_27.wav', 'yes_4.wav', 'yes_14.wav', 'no_15.wav', 'yes_24.wav', 'yes_22.wav', 'yes_20.wav', 'yes_18.wav', 'yes_6.wav', 'no_16.wav', 'no_27.wav', 'no_19.wav', 'no_5.wav', 'yes_10.wav', 'yes_25.wav', 'no_14.wav', 'yes_8.wav', 'no_3.wav', 'yes_28.wav', 'no_8.wav', 'yes_2.wav', 'no_12.wav', 'yes_17.wav', 'yes_12.wav', 'no_4.wav', 'yes_23.wav', 'yes_21.wav', 'yes_9.wav', 'no_28.wav', 'no_2.wav', 'no_21.wav', 'yes_7.wav', 'yes_13.wav', 'yes_29.wav', 'yes_30.wav', 'no_26.wav', 'yes_3.wav', 'no_11.wav', 'no_24.wav', 'no_23.wav', 'no_20.wav', 'no_7.wav', 'yes_26.wav', 'yes_5.wav', 'yes_16.wav', 'no_29.wav', 'no_30.wav', 'no_6.wav', 'yes_11.wav', 'no_25.wav', 'no_10.wav', 'no_13.wav', 'yes_15.wav', 'no_17.wav', 'no_9.wav', 'no_18.wav', 'down_6.wav', 'down_1.wav', 'down_30.wav', 'down_29.wav', 'down_28.wav', 'down_27.wav', 'down_26.wav', 'down_25.wav', 'down_23.wav', 'down_24.wav', 'down_22.wav', 'down_21.wav', 'down_20.wav', 'down_19.wav', 'down_17.wav', 'down_12.wav', 'down_16.wav', 'down_14.wav', 'down_15.wav', 'down_13.wav', 'down_18.wav', 'down_11.wav', 'down_10.wav', 'down_9.wav', 'down_8.wav', 'down_7.wav', 'down_2.wav', 'down_4.wav', 'down_3.wav', 'down_5.wav', 'left_12.wav', 'left_11.wav', 'left_10.wav', 'left_9.wav', 'left_8.wav', 'left_7.wav', 'left_6.wav', 'left_3.wav', 'left_2.wav', 'left_4.wav', 'left_5.wav', 'left_1.wav', 'up_10.wav', 'up_9.wav', 'up_8.wav', 'up_3.wav', 'up_6.wav', 'up_2.wav', 'up_4.wav', 'up_5.wav', 'up_1.wav', 'up_7.wav', 'up_1 copy 4.wav', 'left_30.wav', 'left_28.wav', 'left_27.wav', 'left_29.wav', 'left_26.wav', 'left_25.wav', 'left_22.wav', 'left_23.wav', 'left_24.wav', 'left_21.wav', 'left_20.wav', 'left_19.wav', 'left_14.wav', 'left_17.wav', 'left_16.wav', 'left_15.wav', 'left_18.wav', 'left_13.wav', 'up_30.wav', 'up_29.wav', 'up_28.wav', 'up_26.wav', 'up_23.wav', 'up_24.wav', 'up_25.wav', 'up_22.wav', 'up_27.wav', 'up_21.wav', 'up_19.wav', 'up_18.wav', 'up_17.wav', 'up_15.wav', 'up_16.wav', 'up_14.wav', 'up_13.wav', 'up_11.wav', 'up_12.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Recording"
      ],
      "metadata": {
        "id": "Yq4v5vANXOoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Install sounddevice and Record 30 Samples of Your Voice\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Snqlf2Va43",
        "outputId": "32f708b7-63c1-4ef6-dac6-69438fc82200"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Path to your recordings folder in Drive\n",
        "recordings_path = '/content/drive/MyDrive/Keyword Spotting/Recordings'\n",
        "\n",
        "# Verify the recordings are accessible\n",
        "import os\n",
        "recording_files = os.listdir(recordings_path)\n",
        "print(\"Files in the recordings folder:\", recording_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBgvRzoInn9j",
        "outputId": "9dcefa21-19ae-4ea3-ea89-a1a4b9a073e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the recordings folder: ['no_22.wav', 'yes_1.wav', 'yes_19.wav', 'no_1.wav', 'yes_27.wav', 'yes_4.wav', 'yes_14.wav', 'no_15.wav', 'yes_24.wav', 'yes_22.wav', 'yes_20.wav', 'yes_18.wav', 'yes_6.wav', 'no_16.wav', 'no_27.wav', 'no_19.wav', 'no_5.wav', 'yes_10.wav', 'yes_25.wav', 'no_14.wav', 'yes_8.wav', 'no_3.wav', 'yes_28.wav', 'no_8.wav', 'yes_2.wav', 'no_12.wav', 'yes_17.wav', 'yes_12.wav', 'no_4.wav', 'yes_23.wav', 'yes_21.wav', 'yes_9.wav', 'no_28.wav', 'no_2.wav', 'no_21.wav', 'yes_7.wav', 'yes_13.wav', 'yes_29.wav', 'yes_30.wav', 'no_26.wav', 'yes_3.wav', 'no_11.wav', 'no_24.wav', 'no_23.wav', 'no_20.wav', 'no_7.wav', 'yes_26.wav', 'yes_5.wav', 'yes_16.wav', 'no_29.wav', 'no_30.wav', 'no_6.wav', 'yes_11.wav', 'no_25.wav', 'no_10.wav', 'no_13.wav', 'yes_15.wav', 'no_17.wav', 'no_9.wav', 'no_18.wav', 'down_6.wav', 'down_1.wav', 'down_30.wav', 'down_29.wav', 'down_28.wav', 'down_27.wav', 'down_26.wav', 'down_25.wav', 'down_23.wav', 'down_24.wav', 'down_22.wav', 'down_21.wav', 'down_20.wav', 'down_19.wav', 'down_17.wav', 'down_12.wav', 'down_16.wav', 'down_14.wav', 'down_15.wav', 'down_13.wav', 'down_18.wav', 'down_11.wav', 'down_10.wav', 'down_9.wav', 'down_8.wav', 'down_7.wav', 'down_2.wav', 'down_4.wav', 'down_3.wav', 'down_5.wav', 'left_12.wav', 'left_11.wav', 'left_10.wav', 'left_9.wav', 'left_8.wav', 'left_7.wav', 'left_6.wav', 'left_3.wav', 'left_2.wav', 'left_4.wav', 'left_5.wav', 'left_1.wav', 'up_10.wav', 'up_9.wav', 'up_8.wav', 'up_3.wav', 'up_6.wav', 'up_2.wav', 'up_4.wav', 'up_5.wav', 'up_1.wav', 'up_7.wav', 'up_20.wav', 'left_30.wav', 'left_28.wav', 'left_27.wav', 'left_29.wav', 'left_26.wav', 'left_25.wav', 'left_22.wav', 'left_23.wav', 'left_24.wav', 'left_21.wav', 'left_20.wav', 'left_19.wav', 'left_14.wav', 'left_17.wav', 'left_16.wav', 'left_15.wav', 'left_18.wav', 'left_13.wav', 'up_30.wav', 'up_29.wav', 'up_28.wav', 'up_26.wav', 'up_23.wav', 'up_24.wav', 'up_25.wav', 'up_22.wav', 'up_27.wav', 'up_21.wav', 'up_19.wav', 'up_18.wav', 'up_17.wav', 'up_15.wav', 'up_16.wav', 'up_14.wav', 'up_13.wav', 'up_11.wav', 'up_12.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if all files exist before loading\n",
        "def verify_file_paths(file_list, label):\n",
        "    missing_files = [file for file in file_list if not os.path.isfile(file)]\n",
        "    if missing_files:\n",
        "        print(f\"Missing {label} files:\", missing_files)\n",
        "    else:\n",
        "        print(f\"All {label} files are present.\")\n",
        "\n",
        "# Verify files for each label\n",
        "verify_file_paths(yes_files, \"yes\")\n",
        "verify_file_paths(no_files, \"no\")\n",
        "verify_file_paths(up_files, \"up\")\n",
        "verify_file_paths(down_files, \"down\")\n",
        "verify_file_paths(left_files, \"left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU3dMc3pA-MU",
        "outputId": "cbce9aab-e07f-45f6-bd96-53fe7efbbc10"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All yes files are present.\n",
            "All no files are present.\n",
            "All up files are present.\n",
            "All down files are present.\n",
            "All left files are present.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Fine Tuning"
      ],
      "metadata": {
        "id": "iYrKE4RmXTAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Fine-Tune the Classifier on Your Voice\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "# Load the fine-tune data from \"yes\", \"no\", \"up\", \"down\", and \"left\" recordings\n",
        "yes_files = [os.path.join(recordings_path, f'yes_{i}.wav') for i in range(1, 31)]\n",
        "no_files = [os.path.join(recordings_path, f'no_{i}.wav') for i in range(1, 31)]\n",
        "up_files = [os.path.join(recordings_path, f'up_{i}.wav') for i in range(1, 31)]\n",
        "down_files = [os.path.join(recordings_path, f'down_{i}.wav') for i in range(1, 31)]\n",
        "left_files = [os.path.join(recordings_path, f'left_{i}.wav') for i in range(1, 31)]\n",
        "\n",
        "# Function to pad or truncate audio waveforms\n",
        "fixed_length = 16000\n",
        "\n",
        "def pad_or_truncate(waveform, max_length=fixed_length):\n",
        "    if waveform.size(1) > max_length:\n",
        "        return waveform[:, :max_length]\n",
        "    elif waveform.size(1) < max_length:\n",
        "        pad_amount = max_length - waveform.size(1)\n",
        "        return torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "    return waveform\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Load the audio data and labels for \"yes\", \"no\", \"up\", \"down\", and \"left\"\n",
        "fine_tune_data = []\n",
        "\n",
        "# Process the files for each command and append to fine_tune_data\n",
        "for file, label in zip(yes_files, ['yes'] * len(yes_files)):\n",
        "    waveform, sample_rate = torchaudio.load(file)\n",
        "    waveform = pad_or_truncate(waveform)\n",
        "    spectrogram = mel_spectrogram(waveform)\n",
        "    fine_tune_data.append((spectrogram, label))\n",
        "\n",
        "for file, label in zip(no_files, ['no'] * len(no_files)):\n",
        "    waveform, sample_rate = torchaudio.load(file)\n",
        "    waveform = pad_or_truncate(waveform)\n",
        "    spectrogram = mel_spectrogram(waveform)\n",
        "    fine_tune_data.append((spectrogram, label))\n",
        "\n",
        "for file, label in zip(up_files, ['up'] * len(up_files)):\n",
        "    waveform, sample_rate = torchaudio.load(file)\n",
        "    waveform = pad_or_truncate(waveform)\n",
        "    spectrogram = mel_spectrogram(waveform)\n",
        "    fine_tune_data.append((spectrogram, label))\n",
        "\n",
        "for file, label in zip(down_files, ['down'] * len(down_files)):\n",
        "    waveform, sample_rate = torchaudio.load(file)\n",
        "    waveform = pad_or_truncate(waveform)\n",
        "    spectrogram = mel_spectrogram(waveform)\n",
        "    fine_tune_data.append((spectrogram, label))\n",
        "\n",
        "for file, label in zip(left_files, ['left'] * len(left_files)):\n",
        "    waveform, sample_rate = torchaudio.load(file)\n",
        "    waveform = pad_or_truncate(waveform)\n",
        "    spectrogram = mel_spectrogram(waveform)\n",
        "    fine_tune_data.append((spectrogram, label))\n",
        "\n",
        "# Define the label mapping for all commands\n",
        "label_to_index = {'yes': 0, 'no': 1, 'up': 2, 'down': 3, 'left': 4}\n",
        "\n",
        "# Define a function to convert labels to indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert the data into DataLoader\n",
        "batch_size = 16\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    fine_tune_data, batch_size=batch_size, shuffle=True,\n",
        "    collate_fn=lambda batch: (\n",
        "        torch.stack([item[0] for item in batch]).squeeze(1),\n",
        "        torch.tensor([label_to_tensor(item[1]) for item in batch])\n",
        "    )\n",
        ")\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        waveforms = waveforms.unsqueeze(1).to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        fine_tune_optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        fine_tune_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Fine-tuning Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print(\"Fine-tuning completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVG5XCv2na_q",
        "outputId": "6dbd70c5-9551-43d3-801d-2cf583ce2b6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Fine-tuning Loss: 5.493590688705444\n",
            "Epoch 2/20, Fine-tuning Loss: 3.3784844517707824\n",
            "Epoch 3/20, Fine-tuning Loss: 2.101422643661499\n",
            "Epoch 4/20, Fine-tuning Loss: 1.4913936376571655\n",
            "Epoch 5/20, Fine-tuning Loss: 1.4523927688598632\n",
            "Epoch 6/20, Fine-tuning Loss: 1.4425764203071594\n",
            "Epoch 7/20, Fine-tuning Loss: 1.4275154709815978\n",
            "Epoch 8/20, Fine-tuning Loss: 1.388307511806488\n",
            "Epoch 9/20, Fine-tuning Loss: 1.3768107533454894\n",
            "Epoch 10/20, Fine-tuning Loss: 1.3467698574066163\n",
            "Epoch 11/20, Fine-tuning Loss: 1.3718288898468018\n",
            "Epoch 12/20, Fine-tuning Loss: 1.3270316123962402\n",
            "Epoch 13/20, Fine-tuning Loss: 1.3145939707756042\n",
            "Epoch 14/20, Fine-tuning Loss: 1.2988775014877318\n",
            "Epoch 15/20, Fine-tuning Loss: 1.2989997267723083\n",
            "Epoch 16/20, Fine-tuning Loss: 1.2666441679000855\n",
            "Epoch 17/20, Fine-tuning Loss: 1.315366953611374\n",
            "Epoch 18/20, Fine-tuning Loss: 1.3276232600212097\n",
            "Epoch 19/20, Fine-tuning Loss: 1.2827466249465942\n",
            "Epoch 20/20, Fine-tuning Loss: 1.2284953832626342\n",
            "Fine-tuning completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of samples for each label\n",
        "yes_count = sum([1 for item in fine_tune_data if item[1] == 'yes'])\n",
        "no_count = sum([1 for item in fine_tune_data if item[1] == 'no'])\n",
        "up_count = sum([1 for item in fine_tune_data if item[1] == 'up'])\n",
        "down_count = sum([1 for item in fine_tune_data if item[1] == 'down'])\n",
        "left_count = sum([1 for item in fine_tune_data if item[1] == 'left'])\n",
        "print(f\"Yes count: {yes_count}, No count: {no_count}, Up count: {up_count}, Down count: {down_count}, Left count: {left_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t39x3ZiYrSoq",
        "outputId": "d81edb40-e309-4767-a068-448823752c62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes count: 30, No count: 30, Up count: 30, Down count: 30, Left count: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model and Report the Results\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for waveforms, labels in loader:\n",
        "        waveforms = waveforms.unsqueeze(1).to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        outputs = model(waveforms)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on \"yes\", \"no\", \"up\", \"down\", and \"left\" recordings: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jPWzFJerGB9",
        "outputId": "783e85a7-dbbb-4509-cc14-f81e5f6255d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on \"yes\", \"no\", \"up\", \"down\", and \"left\" recordings: 42.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "# Function to add noise to waveforms for data augmentation\n",
        "def add_noise(waveform, noise_factor=0.005):\n",
        "    noise = torch.randn(waveform.size(), device=waveform.device) * noise_factor\n",
        "    return waveform + noise\n",
        "\n",
        "# Fine-tune the model with a lower learning rate, noise augmentation, and more epochs\n",
        "fine_tune_optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(fine_tune_optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "num_epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Fine-tuning loop with updated learning rate and noise augmentation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        waveforms = waveforms.unsqueeze(1).to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        # Apply noise augmentation to the waveforms\n",
        "        waveforms = add_noise(waveforms)\n",
        "\n",
        "        fine_tune_optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        fine_tune_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Fine-tuning Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print(\"Fine-tuning completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXD-rc2kr_II",
        "outputId": "393b6d0c-fd69-4956-b8c0-7112a2722351"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Fine-tuning Loss: 1.1450494527816772\n",
            "Epoch 2/100, Fine-tuning Loss: 1.145140939950943\n",
            "Epoch 3/100, Fine-tuning Loss: 1.121493148803711\n",
            "Epoch 4/100, Fine-tuning Loss: 1.1284986197948457\n",
            "Epoch 5/100, Fine-tuning Loss: 1.1323743879795074\n",
            "Epoch 6/100, Fine-tuning Loss: 1.1190260708332063\n",
            "Epoch 7/100, Fine-tuning Loss: 1.1425463140010834\n",
            "Epoch 8/100, Fine-tuning Loss: 1.1181742429733277\n",
            "Epoch 9/100, Fine-tuning Loss: 1.1301652193069458\n",
            "Epoch 10/100, Fine-tuning Loss: 1.1142170011997223\n",
            "Epoch 11/100, Fine-tuning Loss: 1.13713396191597\n",
            "Epoch 12/100, Fine-tuning Loss: 1.118468564748764\n",
            "Epoch 13/100, Fine-tuning Loss: 1.1164010107517242\n",
            "Epoch 14/100, Fine-tuning Loss: 1.1228023529052735\n",
            "Epoch 15/100, Fine-tuning Loss: 1.0960364758968353\n",
            "Epoch 16/100, Fine-tuning Loss: 1.1286650061607362\n",
            "Epoch 17/100, Fine-tuning Loss: 1.1319554150104523\n",
            "Epoch 18/100, Fine-tuning Loss: 1.1021289765834807\n",
            "Epoch 19/100, Fine-tuning Loss: 1.1220424473285675\n",
            "Epoch 20/100, Fine-tuning Loss: 1.0919872045516967\n",
            "Epoch 21/100, Fine-tuning Loss: 1.1037737846374511\n",
            "Epoch 22/100, Fine-tuning Loss: 1.116496205329895\n",
            "Epoch 23/100, Fine-tuning Loss: 1.0905216574668883\n",
            "Epoch 24/100, Fine-tuning Loss: 1.1270420968532562\n",
            "Epoch 25/100, Fine-tuning Loss: 1.108832550048828\n",
            "Epoch 26/100, Fine-tuning Loss: 1.0970743358135224\n",
            "Epoch 27/100, Fine-tuning Loss: 1.0980770647525788\n",
            "Epoch 28/100, Fine-tuning Loss: 1.0855262100696563\n",
            "Epoch 29/100, Fine-tuning Loss: 1.1171881139278412\n",
            "Epoch 30/100, Fine-tuning Loss: 1.1020546495914458\n",
            "Epoch 31/100, Fine-tuning Loss: 1.1168303430080413\n",
            "Epoch 32/100, Fine-tuning Loss: 1.1023693799972534\n",
            "Epoch 33/100, Fine-tuning Loss: 1.0975115239620208\n",
            "Epoch 34/100, Fine-tuning Loss: 1.1176027357578278\n",
            "Epoch 35/100, Fine-tuning Loss: 1.1097330391407012\n",
            "Epoch 36/100, Fine-tuning Loss: 1.1021283149719239\n",
            "Epoch 37/100, Fine-tuning Loss: 1.1177226305007935\n",
            "Epoch 38/100, Fine-tuning Loss: 1.1024693608283997\n",
            "Epoch 39/100, Fine-tuning Loss: 1.121358460187912\n",
            "Epoch 40/100, Fine-tuning Loss: 1.1207033753395081\n",
            "Epoch 41/100, Fine-tuning Loss: 1.1012787759304046\n",
            "Epoch 42/100, Fine-tuning Loss: 1.124728798866272\n",
            "Epoch 43/100, Fine-tuning Loss: 1.1315536439418792\n",
            "Epoch 44/100, Fine-tuning Loss: 1.113376760482788\n",
            "Epoch 45/100, Fine-tuning Loss: 1.1191132485866546\n",
            "Epoch 46/100, Fine-tuning Loss: 1.132763135433197\n",
            "Epoch 47/100, Fine-tuning Loss: 1.1179090976715087\n",
            "Epoch 48/100, Fine-tuning Loss: 1.1210821628570558\n",
            "Epoch 49/100, Fine-tuning Loss: 1.1244790732860566\n",
            "Epoch 50/100, Fine-tuning Loss: 1.1156793594360352\n",
            "Epoch 51/100, Fine-tuning Loss: 1.103885579109192\n",
            "Epoch 52/100, Fine-tuning Loss: 1.0941219627857208\n",
            "Epoch 53/100, Fine-tuning Loss: 1.131005984544754\n",
            "Epoch 54/100, Fine-tuning Loss: 1.0895782947540282\n",
            "Epoch 55/100, Fine-tuning Loss: 1.1323895454406738\n",
            "Epoch 56/100, Fine-tuning Loss: 1.1041582345962524\n",
            "Epoch 57/100, Fine-tuning Loss: 1.1318544626235962\n",
            "Epoch 58/100, Fine-tuning Loss: 1.1019151508808136\n",
            "Epoch 59/100, Fine-tuning Loss: 1.1252960979938507\n",
            "Epoch 60/100, Fine-tuning Loss: 1.1303772926330566\n",
            "Epoch 61/100, Fine-tuning Loss: 1.1312127768993379\n",
            "Epoch 62/100, Fine-tuning Loss: 1.1184089303016662\n",
            "Epoch 63/100, Fine-tuning Loss: 1.0932228446006775\n",
            "Epoch 64/100, Fine-tuning Loss: 1.121844631433487\n",
            "Epoch 65/100, Fine-tuning Loss: 1.1201848447322846\n",
            "Epoch 66/100, Fine-tuning Loss: 1.128411728143692\n",
            "Epoch 67/100, Fine-tuning Loss: 1.0962442815303803\n",
            "Epoch 68/100, Fine-tuning Loss: 1.104153972864151\n",
            "Epoch 69/100, Fine-tuning Loss: 1.1013958215713502\n",
            "Epoch 70/100, Fine-tuning Loss: 1.1120005428791047\n",
            "Epoch 71/100, Fine-tuning Loss: 1.1117865920066834\n",
            "Epoch 72/100, Fine-tuning Loss: 1.1188947796821593\n",
            "Epoch 73/100, Fine-tuning Loss: 1.1198678851127624\n",
            "Epoch 74/100, Fine-tuning Loss: 1.0829310834407806\n",
            "Epoch 75/100, Fine-tuning Loss: 1.1335346579551697\n",
            "Epoch 76/100, Fine-tuning Loss: 1.120305496454239\n",
            "Epoch 77/100, Fine-tuning Loss: 1.1312378168106079\n",
            "Epoch 78/100, Fine-tuning Loss: 1.1357884228229522\n",
            "Epoch 79/100, Fine-tuning Loss: 1.1270250737667085\n",
            "Epoch 80/100, Fine-tuning Loss: 1.1318426370620727\n",
            "Epoch 81/100, Fine-tuning Loss: 1.1337417960166931\n",
            "Epoch 82/100, Fine-tuning Loss: 1.098953950405121\n",
            "Epoch 83/100, Fine-tuning Loss: 1.105987161397934\n",
            "Epoch 84/100, Fine-tuning Loss: 1.1087840795516968\n",
            "Epoch 85/100, Fine-tuning Loss: 1.102086114883423\n",
            "Epoch 86/100, Fine-tuning Loss: 1.1156589984893799\n",
            "Epoch 87/100, Fine-tuning Loss: 1.1063821494579316\n",
            "Epoch 88/100, Fine-tuning Loss: 1.1164238393306731\n",
            "Epoch 89/100, Fine-tuning Loss: 1.107345300912857\n",
            "Epoch 90/100, Fine-tuning Loss: 1.118291974067688\n",
            "Epoch 91/100, Fine-tuning Loss: 1.1313012838363647\n",
            "Epoch 92/100, Fine-tuning Loss: 1.108358120918274\n",
            "Epoch 93/100, Fine-tuning Loss: 1.1311262011528016\n",
            "Epoch 94/100, Fine-tuning Loss: 1.1174580097198485\n",
            "Epoch 95/100, Fine-tuning Loss: 1.10975124835968\n",
            "Epoch 96/100, Fine-tuning Loss: 1.1116519570350647\n",
            "Epoch 97/100, Fine-tuning Loss: 1.1174744784832\n",
            "Epoch 98/100, Fine-tuning Loss: 1.1359059274196626\n",
            "Epoch 99/100, Fine-tuning Loss: 1.1050061225891112\n",
            "Epoch 100/100, Fine-tuning Loss: 1.133797711133957\n",
            "Fine-tuning completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model and Report the Results\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for waveforms, labels in loader:\n",
        "        waveforms = waveforms.unsqueeze(1).to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "\n",
        "        outputs = model(waveforms)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on \"yes\", \"no\", \"up\", \"down\", and \"left\" recordings: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jMke1s_sBum",
        "outputId": "04492dd2-01d0-46ff-c06f-0afded9eb202"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on \"yes\", \"no\", \"up\", \"down\", and \"left\" recordings: 92.00% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model weights\n",
        "model_save_path = '/content/drive/MyDrive/Keyword Spotting/fine_tuned_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "twggT5kY1Lyc"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}